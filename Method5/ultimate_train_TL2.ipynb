{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfvcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FlopCountAnalysis\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meinops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Rearrange\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msegmentation_models_pytorch\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msmp\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, Dataset\n",
      "File \u001b[1;32mc:\\Users\\kiree\\anaconda3\\envs\\benchmark\\Lib\\site-packages\\einops\\layers\\torch.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RearrangeMixin, ReduceMixin\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_einmix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _EinmixMixin\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_torch_specific\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m apply_for_scriptable_torch\n\u001b[0;32m      9\u001b[0m __author__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAlex Rogozhnikov\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mRearrange\u001b[39;00m(RearrangeMixin, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n",
      "File \u001b[1;32mc:\\Users\\kiree\\anaconda3\\envs\\benchmark\\Lib\\site-packages\\einops\\_torch_specific.py:127\u001b[0m\n\u001b[0;32m    123\u001b[0m     _ops_were_registered_in_torchdynamo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# module import automatically registers ops in torchdynamo\u001b[39;00m\n\u001b[1;32m--> 127\u001b[0m \u001b[43mallow_ops_in_compiled_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kiree\\anaconda3\\envs\\benchmark\\Lib\\site-packages\\einops\\_torch_specific.py:106\u001b[0m, in \u001b[0;36mallow_ops_in_compiled_graph\u001b[1;34m()\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 106\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m allow_in_graph\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m    108\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_ops_in_compiled_graph failed to import torch: ensure pytorch >=2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mImportWarning\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\kiree\\anaconda3\\envs\\benchmark\\Lib\\site-packages\\torch\\_dynamo\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m allowed_functions, convert_frame, eval_frame, resume_execution\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcode_context\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m code_context\n",
      "File \u001b[1;32mc:\\Users\\kiree\\anaconda3\\envs\\benchmark\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:39\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbytecode_analysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m remove_dead_code, remove_pointless_jumps\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbytecode_transformation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     33\u001b[0m     check_inst_exn_tab_entries_valid,\n\u001b[0;32m     34\u001b[0m     Instruction,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     transform_code_object,\n\u001b[0;32m     38\u001b[0m )\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcache_size\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     40\u001b[0m     CacheSizeRelevantForFrame,\n\u001b[0;32m     41\u001b[0m     compute_cache_size,\n\u001b[0;32m     42\u001b[0m     exceeds_cache_size_limit,\n\u001b[0;32m     43\u001b[0m     is_recompilation,\n\u001b[0;32m     44\u001b[0m )\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meval_frame\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m always_optimize_code_objects, skip_code, TorchPatcher\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     47\u001b[0m     augment_exc_message,\n\u001b[0;32m     48\u001b[0m     BackendCompilerFailed,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m     Unsupported,\n\u001b[0;32m     55\u001b[0m )\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:991\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1087\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1187\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "from einops.layers.torch import Rearrange\n",
    "import segmentation_models_pytorch as smp\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# class PositionalEmbedding(nn.Module):\n",
    "#     def __init__(self, num_patches, projection_dim):\n",
    "#         super(PositionalEmbedding, self).__init__()\n",
    "#         self.embedding = nn.Embedding(num_patches, projection_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         positions = torch.arange(x.shape[1], device=x.device).unsqueeze(0)\n",
    "#         encoded_positions = self.embedding(positions)\n",
    "#         return x + encoded_positions\n",
    "\n",
    "# class MLP(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_units, dropout_rate):\n",
    "#         super(MLP, self).__init__()\n",
    "#         layers = []\n",
    "#         for units in hidden_units:\n",
    "#             layers.extend([\n",
    "#                 nn.Linear(input_dim, units),\n",
    "#                 nn.GELU(),\n",
    "#                 nn.Dropout(dropout_rate)\n",
    "#             ])\n",
    "#             input_dim = units\n",
    "#         self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.mlp(x)\n",
    "\n",
    "class TokenLearner(nn.Module):\n",
    "    def __init__(self, num_tokens):\n",
    "        super(TokenLearner, self).__init__()\n",
    "        self.conv1 = F.tanh(nn.Conv2d(in_channels=1, out_channels=num_tokens, kernel_size=3, padding=1))\n",
    "        self.conv2 = nn.Conv2d(in_channels=num_tokens, out_channels=num_tokens, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=num_tokens, out_channels=num_tokens, kernel_size=3, padding=1)\n",
    "        # self.conv4 = nn.Conv2d(in_channels=num_tokens, out_channels=1, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"TokenLearner\")\n",
    "        # print(x.shape)\n",
    "        # x = F.tanh(self.conv1(x))\n",
    "        x = self.conv1(x)\n",
    "        # print(x.shape)\n",
    "        # x = F.tanh(self.conv2(x))\n",
    "        x = self.conv2(x)\n",
    "        # print(x.shape)\n",
    "        # x = F.tanh(self.conv3(x))\n",
    "        x = self.conv3(x)\n",
    "        # print(x.shape)\n",
    "        # x = F.tanh(self.conv4(x))\n",
    "        # x = self.conv4(x)\n",
    "        # print(x.shape)\n",
    "        # x = x.view(x.size(0), x.size(1), -1)\n",
    "        # print(x.shape)\n",
    "        # x = x.permute(0, 2, 1)  # Permute to match TensorFlow's behavior\n",
    "        # print(x.shape)\n",
    "        # print(\"TokenLearner Done\")\n",
    "        return x\n",
    "\n",
    "# class Transformer(nn.Module):\n",
    "#     def __init__(self, input_dim, num_heads, mlp_units, dropout_rate):\n",
    "#         super(Transformer, self).__init__()\n",
    "#         self.multihead_attention = nn.MultiheadAttention(input_dim, num_heads, dropout=dropout_rate)\n",
    "#         self.mlp = MLP(input_dim, mlp_units, dropout_rate)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # print(\"Transformer\")\n",
    "#         # print(x.shape)\n",
    "#         attention_output, _ = self.multihead_attention(x, x, x)\n",
    "#         # print(x.shape)\n",
    "#         x = x + attention_output\n",
    "#         # print(x.shape)\n",
    "#         x = x + self.mlp(x)\n",
    "#         # print(x.shape)\n",
    "#         return x\n",
    "\n",
    "# class ViTClassifier(nn.Module):\n",
    "#     def __init__(self, image_size, patch_size, num_patches, projection_dim, num_heads, mlp_units, dropout_rate, num_classes, use_token_learner=False, token_learner_units=1):\n",
    "#         super(ViTClassifier, self).__init__()\n",
    "#         self.patch_embedding = nn.Conv2d(in_channels=1, out_channels=projection_dim, kernel_size=patch_size, stride=patch_size)\n",
    "#         self.positional_embedding = PositionalEmbedding(num_patches, projection_dim)\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "#         self.transformer_layers = nn.ModuleList([Transformer(projection_dim, num_heads, mlp_units, dropout_rate) for _ in range(4)])\n",
    "#         self.use_token_learner = use_token_learner\n",
    "#         self.token_learner = TokenLearner(token_learner_units) if use_token_learner else None\n",
    "#         self.layer_norm = nn.LayerNorm(projection_dim)\n",
    "#         # self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "#         # self.classifier = nn.Linear(projection_dim, num_classes)\n",
    "#         self.projection_dim = projection_dim\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # print(x.shape)\n",
    "#         x = self.patch_embedding(x)\n",
    "#         B, _, H, W = x.shape\n",
    "#         # print(x.shape)\n",
    "#         x = x.flatten(2).transpose(1, 2)\n",
    "#         # print(x.shape)\n",
    "#         x = self.positional_embedding(x)\n",
    "#         # print(x.shape)\n",
    "#         x = self.dropout(x)\n",
    "#         # print(x.shape)\n",
    "        \n",
    "#         for idx, transformer in enumerate(self.transformer_layers):\n",
    "#             x = transformer(x)\n",
    "            \n",
    "#             if idx == len(self.transformer_layers) // 2:\n",
    "#                 if self.use_token_learner:\n",
    "#                     x = x.permute(0, 2, 1).unsqueeze(1)  # Adjust shape for TokenLearner\n",
    "#                     # print(x.shape)\n",
    "#                     x = self.token_learner(x)\n",
    "#                     # print(x.shape)\n",
    "#                     x = x.squeeze(1).permute(0, 2, 1)\n",
    "\n",
    "#             # print(x.shape)\n",
    "#         # print(x.shape)\n",
    "\n",
    "#         # if self.use_token_learner:\n",
    "#         #     x = x.permute(0, 2, 1).unsqueeze(1)  # Adjust shape for TokenLearner\n",
    "#         #     # print(x.shape)\n",
    "#         #     x = self.token_learner(x)\n",
    "#         #     # print(x.shape)\n",
    "#         #     x = x.squeeze(1).permute(0, 2, 1)  # Adjust shape back\n",
    "#             # print(x.shape)\n",
    "        \n",
    "#         # print(x.shape)\n",
    "#         return x\n",
    "        \n",
    "#         # x = self.layer_norm(x)\n",
    "#         # x = self.global_avg_pool(x).squeeze(2)\n",
    "#         # x = self.classifier(x)\n",
    "#         # return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, patch_size, num_patches, hidden_dim, num_heads, mlp_dim, dropout=0.1):\n",
    "        super(ViTBlock, self).__init__()\n",
    "        self.patch_embedding = nn.Conv2d(in_channels, out_channels, kernel_size=patch_size, stride=patch_size)\n",
    "        self.position_embedding = nn.Parameter(torch.randn(1, num_patches, out_channels))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, out_channels))\n",
    "        self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(out_channels, num_heads, dim_feedforward=mlp_dim, dropout=dropout, batch_first=True), num_layers=3)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.token_learner = TokenLearner(10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"New Call\")\n",
    "        \n",
    "        # print(\"Encoder 1\", x.shape)\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.patch_embedding(x)  # B, out_channels, H', W'\n",
    "        # print(\"Encoder 2\", x.shape)\n",
    "        \n",
    "        x = x.flatten(2).transpose(1, 2)  # B, (H' * W'), out_channels\n",
    "        # print(\"Encoder 3\", x.shape)\n",
    "        \n",
    "        # cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        # print(\"Encoder 4\", cls_tokens.shape, x.shape)\n",
    "        \n",
    "        # x = torch.cat((cls_tokens, x), dim=1)\n",
    "        # print(\"Encoder 5\", x.shape)\n",
    "\n",
    "        x += self.position_embedding\n",
    "        # print(\"Encoder 6\", x.shape)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        # print(\"Encoder 7\", x.shape)\n",
    "\n",
    "        x = x.permute(0, 2, 1).unsqueeze(1)  # Adjust shape for TokenLearner\n",
    "        # print(x.shape)\n",
    "                    \n",
    "        x = self.token_learner(x)\n",
    "        # print(\"Encoder 9\", x.shape)\n",
    "            \n",
    "        x = x.squeeze(1).permute(0, 2, 1)\n",
    "        \n",
    "        x = self.transformer(x)\n",
    "        # print(\"Encoder 8\", x.shape)\n",
    "\n",
    "\n",
    "        # print(x.shape)        \n",
    "        \n",
    "        return x\n",
    "\n",
    "class ViTUnet(nn.Module):\n",
    "    def __init__(self, num_classes=1, in_channels=1, patch_size=16, vit_hidden_dim=256, vit_num_heads=4, vit_mlp_dim=256, dropout=0.1):\n",
    "        super(ViTUnet, self).__init__()\n",
    "\n",
    "        num_patches = (256 // patch_size) ** 2\n",
    "        self.vit_block1 = ViTBlock(in_channels, vit_hidden_dim, patch_size, num_patches, vit_hidden_dim, vit_num_heads, vit_mlp_dim, dropout)\n",
    "        # self.vit_block2 = ViTBlock(vit_hidden_dim, vit_hidden_dim, patch_size // 2, num_patches // 4, vit_hidden_dim, vit_num_heads, vit_mlp_dim, dropout)\n",
    "        # self.vit_block3 = ViTBlock(vit_hidden_dim, vit_hidden_dim, patch_size // 2, num_patches // 16, vit_hidden_dim, vit_num_heads, vit_mlp_dim, dropout)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"New Iteration\")\n",
    "        # print(\"Decoder 1\", x.shape)\n",
    "        x1 = self.vit_block1(x)\n",
    "        x1 = x1.unsqueeze(1)\n",
    "        # x2 = self.vit_block2(x1.unsqueeze(1)).squeeze(1)\n",
    "        # print(\"Decoder 3\", x2.shape, x2.unsqueeze(1).shape )\n",
    "        # x3 = self.vit_block3(x2.unsqueeze(1)).squeeze(1)\n",
    "        # print(\"Decoder 4\", x3.shape)\n",
    "        # x = F.interpolate(x1.unsqueeze(1), scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        # print(\"Decoder 5\", x.shape)\n",
    "        # x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        # print(\"Decoder 6\", x.shape)\n",
    "        x = self.decoder(x1)\n",
    "        # print(\"Decoder 7\", x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 16\n",
    "num_patches = (256 // patch_size) ** 2\n",
    "num_classes = 1 \n",
    "in_channels = 1 \n",
    "vit_hidden_dim = 256\n",
    "vit_num_heads = 4\n",
    "vit_mlp_dim = 256\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "data = np.load(\"Task06_Lung/Preprocessed/all/data/0.npy\")\n",
    "print(data.shape)\n",
    "# data[0,:,:] = cv2.resize(data[0,:,:], (484,484))\n",
    "data = np.expand_dims(data, 0)\n",
    "print(data,data.shape)\n",
    "plt.imshow(data[0,0,:,:], cmap='bone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.Tensor(data)\n",
    "layer = ViTUnet(num_classes=1, in_channels=1)\n",
    "res1 = layer.vit_block1(data)\n",
    "temp = res1.unsqueeze(1)\n",
    "res2 = layer.decoder(temp)\n",
    "\n",
    "res1 = res1.detach().numpy()\n",
    "res2 = res2.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(res1[0,:,:],cmap='bone')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(res2[0,0,:,:],cmap='bone')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = torch.Tensor(data)\n",
    "# layer = ViTClassifier(image_size=256, patch_size=patch_size, num_patches=num_patches, projection_dim=vit_hidden_dim, num_heads=vit_num_heads,\n",
    "#     mlp_units=[256, 256], dropout_rate=dropout, num_classes=num_classes, use_token_learner=False, token_learner_units=1)\n",
    "# tl_layer = TokenLearner(8)\n",
    "# vit = ViTUnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trans_x = layer(data)\n",
    "# tl_x = tl_layer(trans_x)\n",
    "# decoder = vit.decoder(tl_x)\n",
    "# decoder_2 = vit.decoder(trans_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(trans_x)\n",
    "# print(tl_x)\n",
    "# print(decoder)\n",
    "\n",
    "# trans_x = trans_x.detach().numpy()\n",
    "# tl_x = tl_x.detach().numpy()\n",
    "# decoder = decoder.detach().numpy()\n",
    "# decoder_2 = decoder_2.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(trans_x[0,:,:],cmap='bone')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(tl_x[0,:,:], cmap='bone')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(decoder[0,:,:], cmap='bone')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(decoder_2[0,:,:], cmap='bone')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(trans_x.shape, tl_x.shape, decoder.shape, decoder_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = ['efficientnet-b4', 'efficientnet-b3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark = pd.DataFrame(columns=['model_name', 'epochs', 'gflops', 'dice_score', 'iou_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_path, label_path):\n",
    "        self.data_path = data_path\n",
    "        self.label_path = label_path\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.data_path))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ct = np.load(os.path.join(self.data_path, f\"{idx}.npy\"))\n",
    "        mask = np.load(os.path.join(self.label_path, f\"{idx}.npy\"))\n",
    "\n",
    "        ct = torch.Tensor(ct).to(self.device)\n",
    "        mask = torch.Tensor(mask).to(self.device)\n",
    "\n",
    "        return ct, mask\n",
    "\n",
    "class DataGenerator:\n",
    "    def __init__(self, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.ROOT_DATA_PATH = 'Task06_Lung/Preprocessed/all/data/'\n",
    "        self.ROOT_LABEL_PATH = 'Task06_Lung/Preprocessed/all/label/'\n",
    "        self.TEST_DATA_PATH = 'Task06_Lung/Preprocessed/test/data/'\n",
    "        self.TEST_LABEL_PATH = 'Task06_Lung/Preprocessed/test/label/'\n",
    "\n",
    "    def train_loader(self):\n",
    "        dataset = CustomDataset(self.ROOT_DATA_PATH, self.ROOT_LABEL_PATH)\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def test_loader(self):\n",
    "        dataset = CustomDataset(self.TEST_DATA_PATH, self.TEST_LABEL_PATH)\n",
    "        return DataLoader(dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "data_generator = DataGenerator(batch_size)\n",
    "\n",
    "train_loader = data_generator.train_loader()\n",
    "\n",
    "test_loader = data_generator.test_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceScore(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    class to compute the Dice Loss\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, pred, mask):\n",
    "\n",
    "        #flatten label and prediction tensors\n",
    "        pred = torch.flatten(pred)\n",
    "        mask = torch.flatten(mask)\n",
    "\n",
    "        counter = (pred * mask).sum()  # Counter\n",
    "        denum = pred.sum() + mask.sum()\n",
    "        dice = (2*counter)/denum\n",
    "\n",
    "        return dice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(groundtruth_mask, pred_mask):\n",
    "    intersect = np.sum(pred_mask*groundtruth_mask)\n",
    "    union = np.sum(pred_mask) + np.sum(groundtruth_mask) - intersect\n",
    "    iou = np.mean(intersect/union)\n",
    "    return round(iou, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name in tqdm(models):\n",
    "\n",
    "# #     model =  smp.Unet(\n",
    "# #     encoder_name=name,        \n",
    "# #     encoder_weights=\"imagenet\",     \n",
    "# #     in_channels=1,                  \n",
    "# #     classes=1,                    \n",
    "# # )\n",
    "    \n",
    "model = ViTUnet(num_classes=1, in_channels=1)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "torch.device(device=device)\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_idx, (ct, mask) in enumerate(train_loader):\n",
    "        ct, mask = ct.to(device), mask.to(device)\n",
    "        # print(ct.shape)\n",
    "        # print(mask.shape)\n",
    "\n",
    "        outputs = model(ct)\n",
    "        # print(outputs.shape)\n",
    "\n",
    "        loss = criterion(outputs, mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCH}, Average Loss: {average_loss:.4f}\")\n",
    "\n",
    "preds = []\n",
    "labels = []\n",
    "\n",
    "for batch_idx, (ct, mask) in enumerate(test_loader):\n",
    "\n",
    "    ct, mask = ct.to(device), mask.to(device)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        pred = model(ct)\n",
    "        \n",
    "    pred = pred.cpu().numpy()\n",
    "    mask = mask.cpu().numpy()\n",
    "    # pred = np.where(pred > 0, 1, 0)\n",
    "\n",
    "    preds.append(pred)\n",
    "    labels.append(mask)\n",
    "    \n",
    "preds = np.array(preds)\n",
    "labels = np.array(labels)\n",
    "\n",
    "dice_score = DiceScore()(torch.from_numpy(preds), torch.from_numpy(labels))\n",
    "iou_score = iou(labels, preds)\n",
    "\n",
    "data = torch.rand(4, 1, 256, 256).to(device)\n",
    "flops = FlopCountAnalysis(model, data)\n",
    "print(\"Dice Score:\",dice_score,\"\\nIoU Score:\", iou_score,\"\\nFlops:\", flops.total() // 1e9)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from celluloid import Camera\n",
    "from IPython.display import HTML\n",
    "fig = plt.figure()\n",
    "camera = Camera(fig)\n",
    "\n",
    "for i in range(len(preds)):  # axial view\n",
    "    plt.imshow(preds[i][0, 0, :, :], cmap=\"bone\")\n",
    "    plt.axis(\"off\")\n",
    "    camera.snap()\n",
    "\n",
    "animation = camera.animate()\n",
    "HTML(animation.to_html5_video())\n",
    "plt.show()\n",
    "    # new_row = {'model_name': name, 'epochs':EPOCH, 'gflops': flops, 'dice_score': dice_score, 'iou_score': iou_score}\n",
    "    # benchmark = pd.concat([benchmark, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "HTML(animation.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = 'benchmark.xlsx'\n",
    "# benchmark.to_excel(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
